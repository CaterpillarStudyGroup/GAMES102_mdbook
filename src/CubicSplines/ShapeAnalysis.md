

# Recap: 3D Content Creation    

â€¢ Surface reconstruction    
â€¢ Geometric modeling    
â€¢ Geometry processing    
â€¢ Creative generation    
â€¢ â€¦     

# Analyzing and Understanding 3D Contents    


â€¢ Organize Geometric Data    
â€¢ Understand Structure and Relationships    
â€¢ Understand Semantics and Functionality    
â€¢ Synthesizing New Shapes    
â€¢ â€¦     


# ä¸‰ç»´å‡ ä½•å¤„ç†ï¼šä»å±€éƒ¨åˆ°*å…¨å±€*    

![](../assets/ç†è§£1.png)    



# From lowâ€ to highâ€level processing     

* Local level analysis    
â€¢ purely geometry/contentâ€driven    
â€¢ mathematical formulation of objectives    
â€¢ Examples: curvature and normal estimation, mesh smoothing, simplification, remeshing, parameterizationâ€¦    
* High level analysis    
â€¢ nonâ€local analysis    
â€¢ not easy to formulate objectives mathematically    
â€¢ **Semantics** is hard!    


# Problems of Shape Analysis   


# Understanding Shapes    

* Shape features     
â€¢ Feature points     
â€¢ Feature lines    
â€¢ Saliency    

![](../assets/ç†è§£2.png)    



# Understanding Shapes    

â€¢ Alignment (upright)    

![](../assets/ç†è§£3.png)    


# Understanding Shapes     

â€¢ Shape segmentation (components)    

![](../assets/ç†è§£4.png)    


# Understanding Shapes    

* Coâ€segmentation of a set of shapes     
â€¢ More knowledge can be inferred from multiple shapes rather than an individual shape     

![](../assets/ç†è§£5.png)    



# Understanding Shapes    

â€¢ Labeling    

![](../assets/ç†è§£6.png)    



# Understanding Shapes    

â€¢ Symmetries    

![](../assets/ç†è§£7.png)    



# Understanding Shapes    

â€¢ Skeleton    

![](../assets/ç†è§£8.png)    



# Understanding Shapes    

* Shape matching    
â€¢ Similarity    
â€¢ Correspondences    

![](../assets/ç†è§£9.png)    


# Understanding Shapes   


â€¢ Shape retrieval    

![](../assets/ç†è§£10.png)    



# Understanding Shapes   

â€¢ Classification     

![](../assets/ç†è§£11.png)    



# Understanding Shapes    

â€¢ Structures   
â€¢ Hierarchical structures     

![](../assets/ç†è§£12.png)    


# Understanding Shapes    

â€¢ Functionality    

![](../assets/ç†è§£13.png)    


# Understanding Shapes     

â€¢ Object affordance    

![](../assets/ç†è§£14.png)    


# Understanding Shapes     

* Abstraction of shapes    
â€¢ [Mehra et al. SIGAsia 2009]    

![](../assets/ç†è§£15.png)    

* Understanding assemblies    
â€“ [Mitra et al. SIG 2010]    

![](../assets/ç†è§£15-1.png)    


# Shape Descriptors    


# æ ¸å¿ƒé—®é¢˜ï¼šå½¢çŠ¶è¡¨å¾ï¼ˆæè¿°å­ã€ç‰¹å¾ï¼‰    
(Shape representation/descriptor/feature)    

 - å¦‚ä½•**åº¦é‡**ä¸¤ä¸ªä¸‰ç»´å…ƒç´ çš„ç›¸ä¼¼æ€§ï¼Ÿ    
    - æ•´ä½“å½¢çŠ¶    
     - å…¨å±€æè¿°å­    
 - å±€éƒ¨å½¢çŠ¶    
    - å±€éƒ¨æè¿°    

![](../assets/ç†è§£16.png)    


# ä¾‹å­ï¼šæ¨¡å‹åˆ†å‰²â€æ ¹æ®ç‰¹å¾çš„èšç±»     
(Clustering/Labeling)    

![](../assets/ç†è§£17.png)    



# ä¸‰ç»´æ•°æ®çš„ä¿¡æ¯    

![](../assets/ç†è§£18.png)    


* ç‚¹åæ ‡(x,y,z)    
* å‡ ä½•é‡    
â€¢ é•¿åº¦ã€è§’åº¦ã€é¢ç§¯ã€ ä½“ç§¯    
* å¾®åˆ†é‡     
â€¢ æ³•å‘é‡    
â€¢ æ›²ç‡     
* æ‹“æ‰‘é‡    
â€¢ è¿æ¥å…³ç³»    
â€¢ Laplaceè°±    
* æ˜ å°„åº¦é‡    
â€¢ é›…å¯æ¯”ï¼ˆå˜å½¢é‡ï¼‰   
â€¢ å…±å½¢æ¯”   
* â€¦    


# Shape Descriptors    


* Shape Descriptors     
â€¢ Fixed dimensional vector    
â€¢ Independent of model representation    
â€¢ Easy to match     

![](../assets/ç†è§£19.png)    


# Shape Descriptors    

* Representation:    
â€¢ **What can you represent**?    
â€¢ What are you representing?    

![](../assets/ç†è§£21.png)    

* Matching:    
â€¢ How do you align?   
â€¢ Part or whole matching?    

![](../assets/ç†è§£20-1.png)    



# Shape Descriptors    

* Representation:    
â€¢ What can you represent?    
â€¢ What are you representing?    

![](../assets/ç†è§£21.png)    

* Matching:    
â€¢ How do you align?    
â€¢ Part or whole matching?    

![](../assets/ç†è§£22.png)    


# Shape Descriptors    

* Representation:    
â€¢ What can you represent?    
â€¢ What are you representing?    

![](../assets/ç†è§£23.png)    


* Matching:    
â€¢ **How do you align**?    
â€¢ Part or whole matching?   

![](../assets/ç†è§£24.png)    

   


# Shape Descriptors    


* Representation:    
â€¢ What can you represent?    
â€¢ What are you representing?    

![](../assets/ç†è§£25.png)    


* Matching:   
â€¢ How do you align?   
â€¢ **Part or whole matching**?   

![](../assets/ç†è§£26.png)    



# Extended Gaussian Image     
[Horn, 1984]    


â€¢ Represent a model by a spherical function by binning surface normals      

![](../assets/ç†è§£27.png)    



# Extended Gaussian Image    
[Horn, 1984]

* Properties:    
â€¢ Invertible for convex shapes    
â€¢ 2D array of information    
â€¢ Can be defined for most models    
* Limitations:    
â€¢ Too much information is lost    
â€¢ Normals are sensitive to noise    

![](../assets/ç†è§£28.png)    


# Shape Histograms    
[Ankerst et al., 1999]   

â€¢ Shape descriptor stores a histogram of how much surface resides at different bins in space    

![](../assets/ç†è§£29.png)    



# Gaussian Euclidean Distance Transform    
[Kazhdan et al., 2003]


 - The value at a point is obtained by summing the Gaussian of the closest point on the model surface.    
    &#x2705; Distributes the surface into adjacent bins    
    &#x2705;Maintains highâ€frequency information    

![](../assets/ç†è§£30.png)    



# Spherical Extent Function    
[Vranic et al. 2002]

â€¢ For every view direction, store the distance to the first point a viewer would see when looking at the origin.    

![](../assets/ç†è§£31.png)    



# Spherical Extent Function    

* A model is represented by its starâ€shaped envelope:    
â€¢ The minimal surface containing the model with the property that the center sees every point on the surface    
â€¢ Transforms arbitrary genus models to genusâ€0 surfaces    


![](../assets/ç†è§£32.png)    

![](../assets/ç†è§£33.png)    




# Spherical Extent Function    

* Properties:    
â€¢ Can be defined for most models    
â€¢ Invertible for starâ€shaped models    
â€¢ 2D array of information    
* Limitations:    
â€¢ Distance only measures angular proximity    

![](../assets/ç†è§£34.png)    


# Light Field Descriptor    
[Chen et al. 2003]    


â€¢ For every view direction, store the image the viewer would see when looking at the origin.    

![](../assets/ç†è§£35.png)    



# Light Field Descriptor    

â€¢ Hybrid boundary/volume representation    

![](../assets/ç†è§£36.png)    


# Light Field Descriptor    

 - Properties:    
    - Can be defined for most models    
    - Invertible for starâ€shaped models    
    - 4D array of information    
    - Similarity = sum of area and contour similarities    
      - There is a well defined interior    
      - Can parameterize contours in 2D     

![](../assets/ç†è§£37.png)    

    

# å„ç§äººå·¥å®šä¹‰çš„3Då½¢çŠ¶ç‰¹å¾    

![](../assets/ç†è§£38.png)    



# Methodology    

# Traditional Methods     

![](../assets/ç†è§£39.png)    



# ç‰¹å¾å·¥ç¨‹çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜    

![](../assets/ç†è§£40.png)    



# å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç‰¹å¾ï¼Ÿ   

![](../assets/ç†è§£41.png)    


# æƒ³æ³•ï¼šç¨€ç–å­¦ä¹ é€‰æ‹©åˆé€‚çš„ç‰¹å¾    
[Hu et al. SGP 2012]    

â€¢ ç¨€ç–å­¦ä¹ çš„æœ¬è´¨ï¼šèšç±»    
â€¢ å­ç©ºé—´èšç±»(Subspace clustering)    

![](../assets/ç†è§£42.png)    


# Subspace Clustering     
[Vidal 2010]    

* Input:   
â€¢ high dimensional datasets having low intrinsic dimensions    
â€¢ {\\(x_j\\)}\\(_{j=1,â€¦,N,}x_t âˆˆ â„^D\\)     
 
![](../assets/ç†è§£43.png)    


* Output:    
â€“ multiple lowâ€dimensional linear subspaces    
â€“ ğ¿, \\(ğ‘ƒ_1, ğ‘ƒ_2\\)   


# Formulation: Group Lasso    

* Our solution:    
â€¢ apply subspace clustering in each feature space    
â€¢ add the <u>consistent multiâ€feature penalty</u>       

$$
\underset{W_{1}, \ldots, W_{H}}{\min} \quad \sum_{h=1}^H \mathcal{F}(W_{h})+P_{\text {cons }}\left(W_{1}, W_{2}, \ldots, W_{H}\right)
$$ 

$$
s.t.  \quad W_h \ge 0, \operatorname{diag}(W_{h})=0, \quad h=1,2, \ldots, H
$$

$$
where \quad \mathcal {F} (W_h) =||X_h W_h-X_h|| _F^2 +\lambda\left \|| W_h^T W_h \right \|| _{1,1}
$$


# Consistent multiâ€feature penalty    

1. Find the most similar patch pairs     
2. Corresponding patches need not be similar in all features    

$$
P_{\text {cons }}\left(W_{1}, W_{2}, \ldots, W_{H}\right)=\alpha||W||_{2,1} +  \beta \left \|| W \right \|| _{1,1}
$$


$$
W= \begin{pmatrix} (W_1) _ {11} & (W_1) _ {12} & \cdots & (W_1) _ {N^2}
Â \\\\ (W_2) _ {11} & (W_2)_{12} & \cdots & (W_2) _ {N^2}  
Â \\\\ \vdots  & \vdots  & \ddots & \vdots 
Â \\\\ (W_H) _ {11} & (W_H) _ {12} & \cdots & (W_H) _ {N^2}
\end{pmatrix}
$$

![](../assets/ç†è§£44.png)    


# Results    

![](../assets/ç†è§£45.png)    



# Deep Learning based Methods    


# Handâ€crafted Features are not Enough     

â€¢ â€œHandâ€craftedâ€ feature descriptor need **domain knowledge**    
â€¢ Too many feature descriptor, which is the **best**?    
â€¢ Concatenation of the features may result in **overâ€fitting** in feature space    


Use **deep leaning** to extract good feature descriptors!    



# æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼šç«¯åˆ°ç«¯     

![](../assets/ç†è§£46.png)    


# Deep Neural Networks (DNN)    

![](../assets/ç†è§£47.png)    

Regression problem:     
Input: Given training set \\((x_1, y_1), (x_2, y_2), (x_3, y_3 )\\), â€¦.       
Output: Adjust parameters 0 (for every node) to make:      

$$
h(x_i)\approx y_i
$$



# ä¸‰ç»´æ•°æ®çš„æ·±åº¦å­¦ä¹ çš„ä¸‰ç§æ–¹æ³•     

![](../assets/ç†è§£48.png)    



# å…³äºæ·±åº¦å­¦ä¹     

![](../assets/ç†è§£49.png)    


 - é€šç”¨æ‹Ÿåˆå™¨ï¼ˆè¾ƒå¤§çš„é€¼è¿‘å‡½æ•°ç©ºé—´ï¼‰    
    - åº”ç”¨ä¸‰éƒ¨æ›²    
      - åœ¨å“ªæ‰¾ï¼ˆç½‘ç»œæ„é€ ï¼‰ã€å“ªä¸ªå¥½ï¼ˆæŸå¤±å‡½æ•°ï¼‰ã€æ€ä¹ˆæ‰¾ï¼ˆä¼˜åŒ–ï¼‰    
    - ä»…æ‹Ÿåˆäº†å¤§é‡æ ·æœ¬ï¼šå¯èƒ½åªæ˜¯â€œè™šå‡å…³ç³»â€    
    - å¹¶æ²¡æœ‰â€œç†è§£â€æˆ–â€œè®¤çŸ¥â€çœŸæ­£çš„è§„å¾‹    
    - ä¸å¯è§£é‡Šæ€§    
 - æ€§èƒ½ä¾èµ–è®­ç»ƒæ ·æœ¬ï¼ˆæ•°æ®é›†ï¼‰   
    - å½“æ•°æ®é›†è¶³å¤Ÿå¯†ï¼šè¿‘ä¼¼â€œæœ€è¿‘é‚»â€ç®—æ³•   
    - è®­ç»ƒæ•°æ®é›†ä¸å¤Ÿå®Œå¤‡ï¼šç¼ºä¹æ³›åŒ–èƒ½åŠ›    
    - å¤§éƒ¨åˆ†æ˜¯è¿‡æ‹Ÿåˆ    
 - åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ·±åº¦å­¦ä¹ å¹¶ä¸æ˜¯çœŸæ­£çš„AIï¼Œç¦» çœŸæ­£çš„â€œæ™ºèƒ½â€ä»å¾ˆé¥è¿œ    



# ç¨€ç–å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ï¼šæ®Šé€”åŒå½’     


* æ–¹æ³•çš„ä¸åŒæ€§    
â€¢ å‹ç¼©æ„ŸçŸ¥ï¼šåŸºäº**æ¨¡å‹**çš„ï¼Œæœ‰å¾ˆå¥½çš„ç»“æ„å’Œæ•°å­¦æ¨¡å‹ï¼›æ¥è‡ªäºæ•°å­¦ç†è®ºçš„çªç ´    
â€¢ æ·±åº¦å­¦ä¹ ï¼šåŸºäº**å®è¯**çš„ï¼Œæ¨¡å‹çµæ´»ï¼Œé¡»é€šè¿‡æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ ï¼›æ¥è‡ªäºæ±‚è§£é€Ÿåº¦çš„çªç ´    
* ä¸€è‡´æ€§    
â€¢ ç›®æ ‡ï¼šé«˜ç»´æ•°æ®çš„ä¿¡æ¯(ç‰¹å¾)æå–    
â€¢ ç»“æœï¼šä»å±€éƒ¨ä¿¡æ¯æ¥å¤„ç†å…¨å±€ä¿¡æ¯    
â€¢ ç±»ä¼¼çš„ç½‘ç»œç»“æ„ï¼šæ±‚è§£L1ä¼˜åŒ–çš„IST (Iterative Softâ€Thresholding)ç®—æ³•å®è´¨ä¸Šæ˜¯å¤šå±‚ç½‘ç»œä¼˜åŒ–      


# Trends    


