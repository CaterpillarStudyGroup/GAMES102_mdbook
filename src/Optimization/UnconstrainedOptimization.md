# æ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜    


# Unconstrained Optimization    

$$
\min_xf(x)
$$

â€¢ Gradient descent    
â€¢ Newton    
â€¢ Quasiâ€Newton    
â€¢ Coordinate descent    

![](../assets/ä¼˜åŒ–9.png)   



# æ¢¯åº¦ä¸‹é™æ³•   
(Gradient descent)   

![](../assets/ä¼˜åŒ–10.png)   

$$
x_{k+1}=x_k-\alpha _k\nabla f(x_k)
$$

# æ¢¯åº¦ä¸‹é™æ³•   
(Gradient descent)   

**Line search**    
\\(\Downarrow \\)    
\\(x_{k+1}=x_k-\alpha _k\nabla f(x_k)\\)    

> **Gradient descent**    

# ç‰›é¡¿æ³• (Newtonâ€™s method)    

![](../assets/ä¼˜åŒ–11.png)   


# æ‹Ÿç‰›é¡¿æ³• (Quasiâ€Newton)    

![](../assets/ä¼˜åŒ–12.png)   

* Estimate the Hessian based on previous gradients    
* Recursively inverse Hessian    
 > â€¢ BFGS (Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno algorithm)    
  â€¢ Lâ€BFGS    


# åæ ‡ä¸‹é™æ³• (Coordinate descent)    

**Obj**: minimize\\(_{x,y},ğ¸(ğ‘¥, ğ‘¦)\\)     

â€¢ Alternating variables    

Repeat    
1. \\( y_{k+1}=\min_yE(x_k,y)\\)  
2. \\(x_{k+1}=\min_xE(x,y_{k+1})\\)    


