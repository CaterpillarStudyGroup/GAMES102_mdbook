


# å›é¡¾ï¼šå‚æ•°åŒ–ä¸­çš„ä¼˜åŒ–é—®é¢˜   

$$
\min_{V} E(V)=\sum _{t\in T}(\sigma _1^2+\frac{1}{\sigma _1^2} +\sigma _2^2+\frac{1}{\sigma _2^2}) 
$$

s.t.  \\(\sigma _1\sigma _2>0,\forall t\\)   

![](../assets/ä¼˜åŒ–1.png)   



# å›é¡¾ï¼šå‡ ä½•å¤„ç†ä¸­çš„ä¼˜åŒ–é—®é¢˜   

![](../assets/ä¼˜åŒ–2.png)   


# Geometry Problem and Modeling    

1. Formulate an objective energy \\(E(x)\\)    
2. Define constraints, if apply      
\\(\star \\) Equality / Inequality    
\\(\star \\) Linear / Nonlinear    
3. Numerical optimization     

\begin{array}{cl}
\underset{x \in \mathbb{R}^{n}}{\operatorname{minimize}} & E(x) \\\\
\text { subject to } & c_{1}(x)=d_{1} \\\\
& c_{2}(x)>d_{2}
\end{array}



# Fundamentals   


# ä¼˜åŒ–é—®é¢˜çš„ä¸€èˆ¬å½¢å¼   


é«˜ç»´å®å€¼å‡½æ•°ï¼š\\(f:\mathbb{R}^n\to \mathbb{R}\\)    

\\(\min_x\in \mathbb{R} ^nf(x)\\)    ç›®æ ‡å‡½æ•° or èƒ½é‡å‡½æ•°    

S.t.\\(g(x)=0\\)    ç­‰å¼çº¦æŸ

\\(h(x)\\)\ge 0    ä¸ç­‰å¼çº¦æŸ    


 - Two roles    
    - Client: Which optimization tool is relevant?    
      - ä¸åŒçš„ä¼˜åŒ–**é—®é¢˜**é¡»ç”¨ä¸åŒçš„ä¼˜åŒ–æ–¹æ³•    
    - Designer: Can I design an algorithm for this problem?    
      - ç‰¹å®šçš„ä¼˜åŒ–é—®é¢˜éœ€è¦**è®¾è®¡**ç‰¹å®šçš„ä¼˜åŒ–æ–¹æ³•è¾¾åˆ°æœ€ä½³æ€§èƒ½    
 - Optimization is a huge field.    



# æ¢¯åº¦ (Gradient)ï¼šä¸€é˜¶å¯¼æ•°   

$$
f:\mathbb{R}^n\to \mathbb{R}
$$

$$
\to \nabla f=(\frac{\partial f}{\partial x_1} ,\frac{\partial f}{\partial x_2} ,\cdots ,\frac{\partial f}{\partial x_n})
$$


![](../assets/ä¼˜åŒ–3.png)   


# Jacobian: ä¸€é˜¶â€œå¯¼æ•°â€çŸ©é˜µ    

$$
f:\mathbb{R}^n\to \mathbb{R}^m
$$

$$
\to (Df)_{ij}=\frac{\partial f_i}{\partial x_j} 
$$

![](../assets/ä¼˜åŒ–4.png)   


# Hessian ï¼šäºŒé˜¶â€œå¯¼æ•°â€çŸ©é˜µ   

$$
f:\mathbb{R}^n\to \mathbb{R} \to H_{ij}=\frac{\partial^2 f}{\partial x_i\partial x_j} 
$$

![](../assets/ä¼˜åŒ–5.png)   

$$
f(x)\approx f(x_0)+\nabla f(x_0)^\top (x-x_0)+(x-x_0)^\top Hf(x_0)(x-x_0)
$$


# é©»ç‚¹ï¼ˆCritical pointï¼‰    

$$
\nabla f(x)=0
$$

(unconstrained)    

![](../assets/ä¼˜åŒ–6.png)   


Critical points may not be minima.    


# ä¸€èˆ¬éçº¿æ€§å‡½æ•°çš„æœ€å°å€¼    


* ä»æ— æ³•æ±‚è§£ï¼    
* æ•°å€¼æ±‚è§£    
â€¢ ä»æŸåˆå€¼å¼€å§‹ï¼Œé€æ­¥æ‰¾å…¶é™„è¿‘çš„æå°å€¼    

![](../assets/ä¼˜åŒ–7.png)   


# å‡¸å‡½æ•°çš„é©»ç‚¹å°±æ˜¯æœ€å°å€¼    

![](../assets/ä¼˜åŒ–8.png)   

Numerical Algorithms, Solomon   


# ä¼˜åŒ–é—®é¢˜çš„ç±»å‹   

â€¢ Constrained / Unconstrained    
â€¢ Linear / Nonlinear    
â€¢ Global / Local    
â€¢ Convex / Nonconvex    
â€¢ Continuous / Discrete    
â€¢ Stochastic / Deterministic    
â€¢ Single objective / Multiple objectives    


minimize \\((E_1(x),E_2(x),\cdots ,E_k(x))\\)        


\\(E=\lambda _1E_1+\lambda _2E_2+\cdots +\lambda _kE_k\\)    

# æ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜    


# Unconstrained Optimization    

$$
\min_xf(x)
$$

â€¢ Gradient descent    
â€¢ Newton    
â€¢ Quasiâ€Newton    
â€¢ Coordinate descent    

![](../assets/ä¼˜åŒ–9.png)   



# æ¢¯åº¦ä¸‹é™æ³•   
(Gradient descent)   

![](../assets/ä¼˜åŒ–10.png)   

$$
x_{k+1}=x_k-\alpha _k\nabla f(x_k)
$$

# æ¢¯åº¦ä¸‹é™æ³•   
(Gradient descent)   

**Line search**    
\\(\Downarrow \\)    
\\(x_{k+1}=x_k-\alpha _k\nabla f(x_k)\\)    

> **Gradient descent**    

# ç‰›é¡¿æ³• (Newtonâ€™s method)    

![](../assets/ä¼˜åŒ–11.png)   


# æ‹Ÿç‰›é¡¿æ³• (Quasiâ€Newton)    

![](../assets/ä¼˜åŒ–12.png)   

* Estimate the Hessian based on previous gradients    
* Recursively inverse Hessian    
 > â€¢ BFGS (Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno algorithm)    
  â€¢ Lâ€BFGS    


# åæ ‡ä¸‹é™æ³• (Coordinate descent)    

**Obj**: minimize\\(_{x,y},ğ¸(ğ‘¥, ğ‘¦)\\)     

â€¢ Alternating variables    

Repeat    
1. \\( y_{k+1}=\min_yE(x_k,y)\\)  
2. \\(x_{k+1}=\min_xE(x,y_{k+1})\\)    


# ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜    


# Lagrange Multipliers: Idea   

![](../assets/ä¼˜åŒ–13.png)   


# Lagrange Multipliers: Idea    

![](../assets/ä¼˜åŒ–14.png)   



# Lagrange Multipliers: Idea    

![](../assets/ä¼˜åŒ–15.png)   


# Use of Lagrange Multipliers     

Turns constrained optimization into     
unconstrained rootâ€finding.    

$$
\begin{array}{l}  
  \nabla  \mathbf{f}(x) =\lambda \nabla g(x)  \\\\     
  g(x) = 0 \\    
\end{array} 
$$


# Many Options    

â€¢ **Reparameterization**   
Eliminate constraints to reduce to unconstrained case     

â€¢ **Newtonâ€™s method**    
Approximation: quadratic function with linear constraint    

â€¢ **Penalty method**     
Augment objective with barrier term, e.g. \\(f(x)+\rho |g(x)|\\)    



# Alternating Projection    

![](../assets/ä¼˜åŒ–16.png)   


# Augmented Lagrangians    

![](../assets/ä¼˜åŒ–17.png)   




# Alternating Direction    
Method of Multipliers (ADMM)    

$$
\begin{array}{l}  
  \min_{x,z}& f(x)+g(z)     \\\\  
  s.t. &Ax+Bz=c   \\\\    
\end{array} 
$$

$$
\wedge _\rho (x,z;\lambda )=f(x)+g(z)+\lambda ^\top (Ax+Bz-c)+\frac{\rho }{2}||Ax+Bz-c||_2^2 
$$


<https://web.stanford.edu/~boyd/papers/pdf/admm_slides.pdf>    


# The Art of ADMM â€œSplittingâ€    

![](../assets/ä¼˜åŒ–18.png)   



# ä¸ç­‰å¼çº¦æŸçš„ä¼˜åŒ–é—®é¢˜   

# ä¸€èˆ¬å½¢å¼   

![](../assets/ä¼˜åŒ–19.png)   


# å‡ ä½•è§£é‡Š   

![](../assets/ä¼˜åŒ–20-1.png)   
![](../assets/ä¼˜åŒ–20.png)   



# å‡ ä½•è§£é‡Š   

![](../assets/ä¼˜åŒ–21.png) 




# å‡ ä½•è§£é‡Š    

![](../assets/ä¼˜åŒ–22.png) 


# å‡ ä½•è§£é‡Š    

![](../assets/ä¼˜åŒ–23.png) 


# Firstâ€Order Optimality Conditions   

 - Necessary condition for minimum of   
 ![](../assets/ä¼˜åŒ–24.png) 

 - Langrangian: \\(L(x,\lambda )=f(x)+\sum_{i=1}^{m} \lambda _ig_i(x)\\)  

 - Karushâ€Kuhnâ€Tucker (KKT)     
   conditions for **minimum**  \\(x^*\\)   
 1. Stationarity: \\(\nabla f(x^\ast )+\sum_{i=1}^{m} \lambda _i\nabla g_i(x^\ast )=0\\)    

 2. Primal feasibility:   \\(g_i(x^*)\le 0\\)   
 3. Dual feasibility:  \\(\lambda _i\ge 0\\)    
 4. Complementary slackness:   \\(\lambda _ig_i(x^*)= 0 \\)     

 ![](../assets/ä¼˜åŒ–25.png) 



# Firstâ€Order Optimality Conditions    

![](../assets/ä¼˜åŒ–26.png)    


# ä¼˜åŒ–æ–¹æ³•     
  

![](../assets/ä¼˜åŒ–27.png)    



# Convex Optimization    

# å‡¸å‡½æ•°èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€å°å€¼    


â€¢ Searching globally optimal solutions usually requires convexity!     
â€¢ f convex if:  
\\(f((1-t)a+tb\le (1-t)f(a)+tf(b)\\),    \\(t\in [0,1]\\)    


![](../assets/ä¼˜åŒ–28.png)    



# å‡¸ä¼˜åŒ–é—®é¢˜   

![](../assets/ä¼˜åŒ–29.png)    


# å‡¸ä¼˜åŒ–é—®é¢˜   

![](../assets/ä¼˜åŒ–30.png)    

is convex optimization problem if \\(f(x)\\) and all \\(g_i(x)\\) are convex functions     


<u>consequences</u>   

â€¢ feasible region is convex set    
â€¢ equality constraints can only be affine, i.e. \\(g_i(x)=a^Tx+b\\) since    

$$
g_i(x)=0\Longleftrightarrow 
\begin{cases}
g_i(x) &\le 0\\\\
-g_i(x) &\le 0
\end{cases}
$$

![](../assets/ä¼˜åŒ–31.png)  

# å‡¸ä¼˜åŒ–çš„ä¸»è¦æ–¹æ³•    

![](../assets/ä¼˜åŒ–32.png)    


# å…¶ä»–ä¼˜åŒ–é—®é¢˜    

# Nonlinear Least Squares   

**Obj**: minimize  \\(\sum _ie_i^2(x)\\)    
â€¢ Gaussâ€Newton    

â€¢ Levenbergâ€Marquardt     

$$
\begin{array}{l} 
  \nabla  ^2e_i^2\approx 2(\nabla  e_i)^T\nabla  e_i \\\\
  \nabla^2\approx J^TJ \\ 
\end{array} 
$$



# Mixedâ€Integer Optimization   

![](../assets/ä¼˜åŒ–33.png)    


# å‡ ä½•å¤„ç†ä¸­çš„ä¼˜åŒ–é—®é¢˜    

* å…·æœ‰**ç‰¹æ®Šçš„å‡ ä½•ç»“æ„**ï¼Œå¾€å¾€èƒ½æœ‰ç‰¹æ®Šçš„ä¼˜åŒ–æ–¹æ³•    
â€¢ æ¯”å¦‚ï¼šè§â€œæ›²é¢å‚æ•°åŒ–â€å’Œâ€œå‡ ä½•æ˜ å°„â€ä¸¤èŠ‚è¯¾    

argmin \\(\sum _jf(A_j)\\)       Separable


![](../assets/ä¼˜åŒ–34.png)    


# ä¼˜åŒ–ç›¸å…³çš„è½¯ä»¶   

â€¢ **Eigen** â€” linear algebra    
â€¢ **IPOPT** â€” fast opensource C++ interior point method    
â€¢ **Mosek** â€” commercial (convex) optimization in C, Java, Pythonâ€¦    
â€¢ **Gurobi** â€” commercial mixedâ€integer optimization    
â€¢ **CPLEX** â€” commercial mixedâ€integer optimization    
â€¢ **Matlab** â€” many algorithms, good for prototyping    
â€¢ **CVX** â€” prototyping for convex optimization   
â€¢ **CoMISo** â€” unified interface to above algorithms   
